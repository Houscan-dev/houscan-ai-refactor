import os
import json
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"
INPUT_DIR = "./labeling"
OUTPUT_DIR = "./labeled_json"

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_ID}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True
)

def get_label_from_ai(json_content):
    """AIì—ê²Œ JSON ë‚´ìš©ì„ ë¶„ì„ì‹œì¼œ ì˜ì–´ Key ê°’ì„ ê°€ì§„ ë¼ë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    eligibility = json_content.get("application_eligibility", "")
    precautions = " ".join(json_content.get("precautions", []))
    
    system_prompt = """
    You are a professional housing policy classifier. 
    Analyze the provided content and classify 'User Category' and 'Housing Type'.
    
    [STRICT RULE]
    - Your response MUST be a valid JSON object.
    - Use ONLY the following English keys: 'category_user', 'category_type'.
    - Values for 'category_user' (Multiple choice possible): [ì²­ë…„, ì‹ í˜¼ë¶€ë¶€, ê¸°íƒ€]
    - Values for 'category_type' (Multiple choice possible): [ì•ˆì‹¬ì£¼íƒ, í–‰ë³µì£¼íƒ, ì„ëŒ€ì£¼íƒ, ê¸°íƒ€]

    Classification Guide:
    - Keywords 'ì‹ í˜¼', 'ì‹ ìƒì•„', 'ì˜ˆë¹„ì‹ í˜¼', 'í•œë¶€ëª¨' -> 'ì‹ í˜¼ë¶€ë¶€'
    - Keywords 'ì²­ë…„', 'ëŒ€í•™ìƒ', 'ë§Œ 19ì„¸~39ì„¸' -> 'ì²­ë…„'
    - Keywords 'ë§¤ì…ì„ëŒ€', 'ì „ì„¸ì„ëŒ€', 'êµ­ë¯¼ì„ëŒ€' -> 'ì„ëŒ€ì£¼íƒ'
    - Keywords 'ì²­ë…„ì•ˆì‹¬ì£¼íƒ' -> 'ì•ˆì‹¬ì£¼íƒ'
    """

    user_prompt = f"""
    Content to analyze:
    Eligibility: {eligibility}
    Precautions: {precautions}
    
    Required JSON Format:
    {{
        "category_user": ["value1", "value2"],
        "category_type": ["value1"]
    }}
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=512, 
            do_sample=False,
            temperature=0
        )
    
    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        match = re.search(r"\{[\s\S]*\}", generated_text)
        if match:
            raw_result = json.loads(match.group(0))
            
            final_data = {
                "category_user": raw_result.get("category_user") or raw_result.get("ì‹ ì²­ìœ í˜•") or ["ê¸°íƒ€"],
                "category_type": raw_result.get("category_type") or raw_result.get("ì§‘ìœ í˜•") or ["ê¸°íƒ€"]
            }
            return final_data
    except Exception as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
    return {"category_user": ["ê¸°íƒ€"], "category_type": ["ê¸°íƒ€"]}

print("ğŸš€ ìë™ ë¼ë²¨ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

json_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.json')]

for file_name in json_files:
    file_path = os.path.join(INPUT_DIR, file_name)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ‘‰ ì²˜ë¦¬ ì¤‘: {file_name}")
        
        labels = get_label_from_ai(data)
        
        data["category_user"] = labels["category_user"]
        data["category_type"] = labels["category_type"]
        
        output_path = os.path.join(OUTPUT_DIR, file_name)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            
    except Exception as e:
        print(f"âŒ {file_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

print(f"\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì´ {len(json_files)}ê°œì˜ íŒŒì¼ì´ '{OUTPUT_DIR}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")